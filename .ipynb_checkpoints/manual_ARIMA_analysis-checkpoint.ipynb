{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from matplotlib.pyplot import rcParams\n",
    "rcParams['figure.figsize']=10,6\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from config import api_key\n",
    "import eia\n",
    "import datetime\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part One: Import All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'021bf1d89256205c8528b6d4475485b3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test if api_key variable works\n",
    "# (remove this line output when making public)\n",
    "api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save variable to call in api data w/ key\n",
    "api = eia.API(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test one api ending; import to pandas dataframe\n",
    "test_import = api.data_by_series(series='TOTAL.TETCBUS.M')\n",
    "eia_df = pd.DataFrame(test_import)\n",
    "eia_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create numeric index (unique identifier)\n",
    "eia_df = eia_df.reset_index()\n",
    "eia_df = eia_df.rename(columns={\"index\":\"Time\"})\n",
    "eia_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part Two: Reshape data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object for existing timestamp column and an empty list for converted timestamp\n",
    "timestamp = eia_df[\"Time\"]\n",
    "datetime_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill empty list with dates converted to datetime\n",
    "for i in timestamp:\n",
    "    i = i.replace(\" \", \"-\")\n",
    "    i = i + \"01\"\n",
    "    i = pd.to_datetime(i, infer_datetime_format = True)\n",
    "    datetime_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add datetime array to total consumption df\n",
    "eia_df['Date'] = datetime_list\n",
    "eia_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new dataframe with just datetime(as index) and usage columns\n",
    "raw_df = eia_df[[\"Date\", \"Total Primary Energy Consumption, Monthly (Trillion Btu)\"]]\n",
    "raw_df = raw_df.set_index(\"Date\")\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start time series testing here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.plotly as ply\n",
    "import cufflinks as cf\n",
    "from plotly.plotly import plot_mpl\n",
    "import plotly.graph_objs as go\n",
    "plotly.tools.set_credentials_file(username='disco_nap', api_key='84gBOgUqBKJnD1eR2tFV')\n",
    "\n",
    "raw_df.iplot(title=\"Energy Consumption Jan 1973-May 2018\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decomposition\n",
    "### Here we can see there is an upward trend. We can use statsmodels to perform a decomposition of this time series. The decomposition of time series is a statistical task that deconstructs a time series into several components, each representing one of the underlying categories of patterns. With statsmodels we will be able to see the trend, seasonal, and residual components of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import __version__\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "result = seasonal_decompose(raw_df, model='multiplicative')\n",
    "fig = result.plot()\n",
    "url = plot_mpl(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.tools as tls\n",
    "tls.embed(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below we perform a grid search for the optimal p, d, q parameters-\n",
    "#### We want the lowes AIC score. AIC is an estimator of the relative quality of statistical models for a given set of data. The AIC value will allow us to compare how well a model fits the data and takes into account the complexity of a model, so models that have a better fit while using fewer features will receive a better (lower) AIC score than similar models that utilize more features.\n",
    "\n",
    "-- it runs this and automatically chooses the best one. In this case the best one is the AIC score of 7068.90 so we use the order parameters of (1,1,1) and seasonal order parameters of (2,1,1,12)\n",
    "\n",
    "-- You can see these results below in the variable \"results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a stepwise model to try different lag scenerios to find the optimal p, d, q value for trend and seasonality\n",
    "\n",
    "from pyramid.arima import auto_arima\n",
    "stepwise_model = auto_arima(raw_df, start_p=1, start_q=1,\n",
    "                           max_p=3, max_q=3, m=12,\n",
    "                           start_P=0, seasonal=True,\n",
    "                           d=1, D=1, trace=True,\n",
    "                           error_action='ignore',  \n",
    "                           suppress_warnings=True, \n",
    "                           stepwise=True)\n",
    "print(stepwise_model.aic())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will print the optimal p, d, q values for trend and seasonal\n",
    "print(stepwise_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are training on the data from 1973 through the end of 2015. The rest is test data.\n",
    "train = raw_df.loc['1973-01-01':'2015-12-01']\n",
    "test = raw_df.loc['2016-01-01':]\n",
    "len(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the ARIMA model\n",
    "results=stepwise_model.fit(train)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "future_forecast = stepwise_model.predict(n_periods=33)\n",
    "# 21 is the length of the test data\n",
    "# below prints the \"forecast\" for the test data\n",
    "print(future_forecast)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# we graph the actual data on top of the forecasted data that we created. It follows the same trend so we know that it was successful \n",
    "future_forecast = pd.DataFrame(future_forecast,index = test.index,columns=['Prediction'])\n",
    "pd.concat([test,future_forecast],axis=1).iplot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate % error of 2017 predictions using ARIMA\n",
    "\n",
    "actual = sum(test[\"Total Primary Energy Consumption, Monthly (Trillion Btu)\"])\n",
    "prediction = sum(future_forecast['Prediction'])\n",
    "\n",
    "error = (actual - prediction) / actual\n",
    "percent_error = error*100\n",
    "\n",
    "print(f\"ARIMA prediction error: {percent_error}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the SARIMAX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "# SARIMAX= Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors model\n",
    "\n",
    "model=sm.tsa.statespace.SARIMAX(raw_df[\"Total Primary Energy Consumption, Monthly (Trillion Btu)\"],order=(1,1,1), seasonal_order=(2,1,1,12))\n",
    "results=model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is predicting the data based on our test data \n",
    "raw_df[\"Forecast\"]=results.predict(start=500, end=549, dynamic=True)\n",
    "raw_df[[\"Total Primary Energy Consumption, Monthly (Trillion Btu)\", \"Forecast\"]].plot(figsize=(30,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df[\"Forecast\"]=results.predict(start=500, end=549, dynamic=True)\n",
    "raw_df[[\"Total Primary Energy Consumption, Monthly (Trillion Btu)\", \"Forecast\"]][500:].plot(figsize=(30,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = raw_df[\"Forecast\"].tail(49)\n",
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = raw_df[\"Total Primary Energy Consumption, Monthly (Trillion Btu)\"][500:]\n",
    "sum(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_prediction = sum(predictions)\n",
    "actual = sum(raw_df[\"Total Primary Energy Consumption, Monthly (Trillion Btu)\"][500:])\n",
    "\n",
    "error = (actual - s_prediction) / actual\n",
    "percent_error = error*100\n",
    "\n",
    "print(f\"SARIMA prediction error: {percent_error}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarima has the closest prediction, so we will move forward with future predictions using SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 500 refers to how many months you will predict into the future\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "future_dates=[raw_df.index[-1]+DateOffset(months=x) for x in range (0,500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "futures_datest_df=pd.DataFrame(index=future_dates[1:], columns=raw_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "futures_datest_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_df=pd.concat([raw_df, futures_datest_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this chart below is projecting into the future. The start value is when you start predicting.\n",
    "# 549 is the length of our data so I am predicting 151 months out. This number is easily adjusted but you have to adjust the number above (currently 48)\n",
    "future_df[\"Forecast\"]=results.predict(start= 548, end= 900, dynamic=True)\n",
    "future_df[[\"Total Primary Energy Consumption, Monthly (Trillion Btu)\", \"Forecast\"]][500:].plot(figsize=(30,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_df[\"Forecast\"]=results.predict(start= 540, end= 3000, dynamic=True)\n",
    "future_df[[\"Total Primary Energy Consumption, Monthly (Trillion Btu)\", \"Forecast\"]].plot(figsize=(30,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual ARIMA Analysis Below\n",
    "## Prior to importing autoarima, we tried several methods of achieving stationarity, so that we could pick the optimal p, d, q values.\n",
    "\n",
    "## The steps for manual testing are below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Test stationarity\n",
    "### If mean is not constant, data is not stationary. Scale the data in various ways to figure out the best way to achieve stationarity.\n",
    "#### We test for stationarity in two ways: a visual test and a statistical test\n",
    "- VISUAL TEST: plot rolling mean and stdev for a visual of whether data is stationary\n",
    "    - if data is stationary, it the mean should be close to a straight line with no slope\n",
    "- STATS TEST: run Dickey-Fuller test for statistical confirmation of whether data is stationary\n",
    "    - if data is stationary, the P-value should be < 0.05 and the absolute value of the Test Statistic should be greater than the absolute value of the Critical Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to create rolling mean and stdev, plot for visual test, and DF for stat test\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "def test_stationarity(timeseries):\n",
    "    \n",
    "    #Determine rolling stats\n",
    "    movingAverage=timeseries.rolling(window=12).mean()\n",
    "    movingStd=timeseries.rolling(window=12).std()\n",
    "    \n",
    "    #plot rolling stats\n",
    "    orig=plt.plot(timeseries, color=\"blue\", label=\"Original\")\n",
    "    mean=plt.plot(movingAverage, color=\"red\", label=\"Rolling Mean\")\n",
    "    std=plt.plot(movingStd, color=\"black\", label=\"Rolling STD\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Rolling Mean and STD\")\n",
    "    plt.show(block=False)\n",
    "     \n",
    "    #perform Dickey-Fuller test\n",
    "    print(\"Results of Dickey Fuller test:\")\n",
    "    dftest = adfuller(timeseries[\"Total Primary Energy Consumption, Monthly (Trillion Btu)\"], autolag = 'AIC')\n",
    "    dfoutput=pd.Series(dftest[0:4], index=[\"Test Statistic\", \"P-Value\",'# Lags Used',\"# of Obsv Used\"])\n",
    "    for key, value in dftest[4].items():\n",
    "        dfoutput[\"Critical Value (%s)\"%key]=value\n",
    "    print(dfoutput)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this function on scaled data:\n",
    "\n",
    "#### https://otexts.com/fpp2/stationarity.html\n",
    "- Raw data (for baseline)\n",
    "- Log scale\n",
    "- Log scale minus moving average\n",
    "- Differencing:\n",
    "    - on Log scale\n",
    "    - on Log scale minus moving average\n",
    "- Exponential weighting\n",
    "- Differencing (log scale) + Exponential weighting\n",
    "- Seasonal Decomposition\n",
    "\n",
    "This shows one way to make a non-stationary time series stationary — compute the differences between consecutive observations. This is known as differencing.\n",
    "\n",
    "Transformations such as logarithms can help to stabilise the variance of a time series. Differencing can help stabilise the mean of a time series by removing changes in the level of a time series, and therefore eliminating (or reducing) trend and seasonality.\n",
    "\n",
    "As well as looking at the time plot of the data, the ACF plot is also useful for identifying non-stationary time series. For a stationary time series, the ACF will drop to zero relatively quickly, while the ACF of non-stationary data decreases slowly. Also, for non-stationary data, the value of r1 is often large and positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how we know the raw data is not stationary\n",
    "\n",
    "test_stationarity(raw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new df with log scaled data\n",
    "raw_df_logScale=np.log(raw_df)\n",
    "\n",
    "#create a rolling mean of log scaled data\n",
    "raw_df_log_smooth = raw_df_logScale.rolling(window = 12).mean()\n",
    "\n",
    "#plot both\n",
    "plt.plot(raw_df_log_smooth, color = 'red')\n",
    "plt.plot(raw_df_logScale)\n",
    "plt.show()\n",
    "\n",
    "# this data does not appear stationary\n",
    "# sometimes it is easier to visualize without the stdev because the y-scale adjusts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stationarity(raw_df_logScale)\n",
    "# DF confirms this data is not stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with log scale minus moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset rolling mean and stdev of log scaled data\n",
    "movingAverage = raw_df_logScale.rolling(window=12).mean()\n",
    "movingStd = raw_df_logScale.rolling(window=12).std()\n",
    "\n",
    "# create df of Log - Moving Avg\n",
    "logScaleMinusMovingAverage_df = raw_df_logScale - movingAverage\n",
    "logScaleMinusMovingAverage_df.dropna(inplace=True)\n",
    "logScaleMinusMovingAverage_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset rolling mean of Log - Moving Avg \n",
    "movingAverage = logScaleMinusMovingAverage_df.rolling(window=12).mean()\n",
    "\n",
    "# plot both\n",
    "plt.plot(movingAverage, color = 'red')\n",
    "plt.plot(logScaleMinusMovingAverage_df)\n",
    "plt.show()\n",
    "\n",
    "# this data appears stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stationarity(logScaleMinusMovingAverage_df)\n",
    "# DF confirms this data is stationary\n",
    "# this one is the best P-value so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Seasonal Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "decomposition = seasonal_decompose(raw_df)\n",
    "\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "\n",
    "plt.subplot(411)\n",
    "plt.plot(raw_df, label='Original')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label='Trend')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal,label='Seasonality')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(414)\n",
    "plt.plot(residual, label='Residuals')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test stationarity on residual plot after trend and seasonality has been removed \n",
    "\n",
    "raw_df_decompose = residual\n",
    "raw_df_decompose.dropna(inplace=True)\n",
    "test_stationarity(raw_df_log_decompose)\n",
    "\n",
    "# DF shows this is by far the highest confidence model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The seasonal decomposition is the best way to achieve stationarity for this dataset. We can use this scale to determine the best p, d, and q values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin forecasting here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA model\n",
    "\n",
    "ARIMA stands for Auto-Regressive Integrated Moving Averages. The ARIMA forecasting for a stationary time series is nothing but a linear (like a linear regression) equation. The predictors depend on the parameters (p,d,q) of the ARIMA model:\n",
    "\n",
    "1. Number of AR (Auto-Regressive) terms (p): AR terms are just lags of dependent variable. For instance if p is 5, the predictors for x(t) will be x(t-1)….x(t-5).\n",
    "2. Number of MA (Moving Average) terms (q): MA terms are lagged forecast errors in prediction equation. For instance if q is 5, the predictors for x(t) will be e(t-1)….e(t-5) where e(i) is the difference between the moving average at ith instant and actual value.\n",
    "3. Number of Differences (d): These are the number of nonseasonal differences\n",
    "\n",
    "#### Selecting p, q, and d values\n",
    "\n",
    "1. Autocorrelation Function (ACF): It is a measure of the correlation between the the TS with a lagged version of itself. For instance at lag 5, ACF would compare series at time instant ‘t1’…’t2’ with series at instant ‘t1-5’…’t2-5’ (t1-5 and t2 being end points).\n",
    "2. Partial Autocorrelation Function (PACF): This measures the correlation between the TS with a lagged version of itself but after eliminating the variations already explained by the intervening comparisons. Eg at lag 5, it will check the correlation but remove the effects already explained by lags 1 to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACF and PACF plots:\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "\n",
    "lag_acf = acf(raw_df_log_decompose, nlags=20)\n",
    "lag_pacf = pacf(raw_df_log_decompose, nlags=20, method = 'ols')\n",
    "#ols is 'ordinary least squares' method\n",
    "\n",
    "#Plot ACF:\n",
    "plt.subplot(121)\n",
    "plt.plot(lag_acf)\n",
    "plt.axhline(y=0, linestyle='-', color='gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(raw_df_log_decompose)), linestyle='-', color='gray')\n",
    "plt.axhline(y=1.96/np.sqrt(len(raw_df_log_decompose)), linestyle='-', color='gray')\n",
    "plt.title('Autocorrelation Function')\n",
    "\n",
    "#Plot PACF:\n",
    "plt.subplot(122)\n",
    "plt.plot(lag_pacf)\n",
    "plt.axhline(y=0, linestyle='-', color='gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(raw_df_log_decompose)), linestyle='-', color='gray')\n",
    "plt.axhline(y=1.96/np.sqrt(len(raw_df_log_decompose)), linestyle='-', color='gray')\n",
    "plt.title('Partial Autocorrelation Function')\n",
    "plt.tight_layout()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData]",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
